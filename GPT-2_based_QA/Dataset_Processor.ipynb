{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset_Processor",
      "provenance": [],
      "authorship_tag": "ABX9TyOD4kOpTWsljGYlvyHRQHnW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENHG8KfrTY4T"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW4EKAKlMeHc"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJAbjm-Tdhg"
      },
      "source": [
        "## Transforming the QA dataset for the language modeling task\n",
        "\n",
        "Uploading the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjlYee6EMzEk"
      },
      "source": [
        "path = Path('qa_dataset.json')\n",
        "data = json.loads(path.read_text(encoding='utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi_t6BWeTw8Z"
      },
      "source": [
        "Creating a new file with question and answers only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltwHVJX7M6cq"
      },
      "source": [
        "with open(r\"dataset.txt\", \"w\") as file:\n",
        "    for group in data['data']:\n",
        "      for qas in group['paragraphs']:\n",
        "        for q in qas['qas']:\n",
        "          file.write(q['question'] + '\\n')\n",
        "          file.write(q['answers'][0]['text'] + '\\n')\n",
        "          file.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umEMYB9gT6OA"
      },
      "source": [
        "Splitting the QA pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfpN6jsaM7iz"
      },
      "source": [
        "path = Path('dataset.txt')\n",
        "data = path.read_text(encoding='utf-8').split('\\n\\n')[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lk3cEyCUTx2"
      },
      "source": [
        "Splitting the data into train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep7tybjZO08O"
      },
      "source": [
        "train, val = train_test_split(data, test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5G6_DeWUYll"
      },
      "source": [
        "Saving both sets as *.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9emucKTPpXB"
      },
      "source": [
        "with open(r\"train.txt\", \"w\") as file:\n",
        "  for pair in train:\n",
        "    file.write(pair + '\\n')\n",
        "    file.write('\\n')\n",
        "\n",
        "with open(r\"val.txt\", \"w\") as file:\n",
        "  for pair in val:\n",
        "    file.write(pair + '\\n')\n",
        "    file.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
